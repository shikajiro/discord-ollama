import { ChatResponse } from "ollama"
import { ChatParams } from "../index.js"
import { AbortableAsyncIterator } from "ollama/src/utils.js"

/**
 * Method to query the Ollama client for async generation
 * @param params 
 * @returns AsyncIterator<ChatResponse> generated by the Ollama client
 */
export async function streamResponse(params: ChatParams): Promise<AbortableAsyncIterator<ChatResponse>> {
    const messages = params.msgHist.map(msg => ({
        role: msg.role,
        content: msg.username && msg.role === 'user' ? `${msg.username}: ${msg.content}` : msg.content,
        images: msg.images
    }))
    
    if (params.systemPrompt) {
        messages.unshift({
            role: 'system',
            content: params.systemPrompt,
            images: []
        })
    }
    
    return await params.ollama.chat({
        model: params.model,
        messages: messages,
        options: {
            mirostat: 1,
            mirostat_tau: 2.0,
            top_k: 70
        },
        stream: true
    }) as unknown as AbortableAsyncIterator<ChatResponse>
}

/**
 * Method to query the Ollama client for a block response 
 * @param params parameters to query the client
 * @returns ChatResponse generated by the Ollama client
 */
export async function blockResponse(params: ChatParams): Promise<ChatResponse> {
    const messages = params.msgHist.map(msg => ({
        role: msg.role,
        content: msg.username && msg.role === 'user' ? `${msg.username}: ${msg.content}` : msg.content,
        images: msg.images
    }))
    
    if (params.systemPrompt) {
        messages.unshift({
            role: 'system',
            content: params.systemPrompt,
            images: []
        })
    }
    
    return await params.ollama.chat({
        model: params.model,
        messages: messages,
        options: {
            mirostat: 1,
            mirostat_tau: 2.0,
            top_k: 70
        },
        stream: false
    })
}